---
title: "Main Notebook"
output: html_notebook
---

```{r}
list.of.packages <- c("data.table","FeatureHashing","xgboost","dplyr","Matrix","caret","randomForest","lightgbm","magrittr","data.table")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages))
  {
   install.packages(new.packages)
  }
```


```{r}
#write library
library(lightgbm)
library(magrittr)
library(data.table)
library(randomForest)

```

```{r}
train_num<- fread("../output/train_num.csv") %>% as.data.frame()
test_num<- fread("../output/test_num.csv") %>% as.data.frame()
train_num_runique<- fread("../output/train_num_runique.csv") %>% as.data.frame()
test_num<- fread("../output/test_num_runique.csv") %>% as.data.frame()


test = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_test.csv") %>% as.data.frame()
train = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_train.csv") %>% as.data.frame()
people = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/people.csv") %>% as.data.frame()

names(people)[2:length(names(people))]=paste0('people_',names(people)[2:length(names(people))])
p_logi <- names(people)[which(sapply(people, is.logical))]
for (col in p_logi) {
  set(people, j = col, value = as.numeric(people[[col]]))
}
train_agg <- merge(train, people, by = "people_id", all.x = T)


```


Part 1: Model Selction ######Need to train the best parameters(plz write thsi process into function)

Model1: SVM

Model2: Multinomial Log Linear

```{r}
library(nnet)
multinom_data <- train_num


#SUBSET TO RELEVANT VARIABLES
#unfortunately, multinom() cannot handle more than 1024 categories of a variable 
multinom_data <- subset(data,select=-c(X,people_id,activity_id,date,people_date))


#SPLIT INTO TRAIN AND TEST
set.seed(2017)
multinom_index <- sample(1:nrow(multinom_data), size=0.7*nrow(multinom_data)) 
multinom_train_data <- multinom_data[index,] 
multinom_test_data <- multinom_data[-index,]


#TRAIN THE MODEL + OBTAIN RUN TIME
multinom_train <- function(multinom_train_data){
  multinom_fit <- multinom(formula = as.factor(outcome) ~ .,
                           data=multinom_train_data, MaxNWts = 10000, maxit = 500)
  top_models = varImp(multinom_fit)
  top_models$variables = row.names(top_models)
  top_models = top_models[order(-top_models$Overall),]
  return(list(fit=multinom_fit, top=top_models))
}
system.time(multinomfit_train <- multinom_train(train_data))


#TEST THE MODEL
multinom_test <- function(multinom_test_data, fit){
  multinom_pred = predict(fit,newdata=multinom_test_data)
  return(multinom_pred)
}

# run it:
multinomtest_result = multinom_test(multinom_test_data,multinomfit_train$fit)
postResample(multinom_test_data$outcome,multinomtest_result)



Model3: Random Forest:

```{r}

  library(randomForest)

  train_num = train_num[, !names(train_num) %in% c("V1")]
  length(train_num)
  fit <- randomForest(as.factor(outcome)~., data= train_num, mtry=8,
                      importance=TRUE, 
                      ntree=25)
  
```


Model4: GBM

Model5: LightGBM:

```{r}
DT2mat <- function(DT, low_mem = FALSE, collect = 0, silent = TRUE) {
  
  # Can't initialize lower
  mat_sub <- matrix(rep(FALSE, nrow(DT) * ncol(DT)), ncol = ncol(DT))
  
  cols <- copy(colnames(DT))
  colnames(mat_sub) <- cols
  
  if (collect == 0) {
    # Don't garbage collect
    
    if (low_mem == TRUE) {
      # delete old
      for (i in cols) {
        mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
        set(DT, j = i, value = NULL)
      }
      
    } else {
      # not low mem
      for (i in cols) {
        mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
      }
      
    }
    
  } else {
    # Do garbage collect
    
    if (silent == FALSE) {
      # not silent
      
      if (low_mem == TRUE) {
        # delete old
        j <- 1
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          set(DT, j = i, value = NULL)
          if (!(j %% collect)) {gc(verbose = FALSE); cat("\rIteration: ", j, ".", sep = "")}
        }
        
      } else {
        # not low mem
        j <- 0
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          if (!(j %% collect)) {gc(verbose = FALSE); cat("\rIteration: ", j, ".", sep = "")}
        }
        
      }
      
    } else {
      
      if (low_mem == TRUE) {
        # delete old
        j <- 1
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          set(DT, j = i, value = NULL)
          if (!(j %% collect)) {gc(verbose = FALSE)}
        }
        
      } else {
        # not fast
        j <- 0
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          if (!(j %% collect)) {gc(verbose = FALSE)}
        }
        
      }
      
    }
  }
  
  return(mat_sub)
  
}
```

```{r}




#manual treatment:
train_num$people_id <- as.numeric(as.factor(train_num$people_id)) - 1
train_num$activity_id <- as.numeric(as.factor(train_num$activity_id)) - 1
train_num$date <- as.numeric(as.factor(train_num$date)) - 1
train_num$people_date <- as.numeric(as.factor(train_num$people_date)) - 1
train_num = train_num[, !names(train_num) %in% c("V1")]
train_num_data= train_num[, !names(train_num) %in% c("outcome")]

#automated treatment
rules <- lgb.prepare_rules(data = train_agg)
data_train <- bank_rules$data


temp_train <- lgb.Dataset(DT2mat(train_num_data), label=train_num$outcome, free_raw_data = FALSE, colnames = colnames(train_num_data), categorical_feature = c(1, 2, 3, 18))

bst <- lightgbm(data = temp_train,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
                objective = "binary")

pred <- predict(bst, DT2mat(train_num_data))
err <- mean(as.numeric(pred > 0.5) != train_num$outcome)
print(paste("test-error=", err))


print("Training lightgbm with lgb.Dataset")
dtrain <- lgb.Dataset(data = test_num[, !names(test_num) %in% c("V1", "outcome")],
                      label = test_num$outcome)
bst <- lightgbm(data = dtrain,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
objective = "binary")

bst <- lightgbm(data = as.matrix(train_num$data),
                label = train$label,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
objective = "binary")
```

Model6: Xgboost

Model7: Neural Network

Part 2: Feature Selection

1. Feature Importance

2. PCA

3.Bag of words techniques

Part 3: Retrain model
