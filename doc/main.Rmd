---
title: "Main Notebook"
output: html_notebook
---

In this project, we conducted a couple of machine learning algorithms on Red Hat Business data set, which is pretty large(almost two millions*56). We mainly compared SVM(linear and nonlinear), Random Forest, Light GBM, Xgboost, Neural Network, Multinomial log-linear Model. And it proved Xgoost and Light GBM have the highest accuracy around 98%.

Step 0: Load the packages
```{r}
list.of.packages <- c("data.table","FeatureHashing","xgboost","dplyr","Matrix","caret","randomForest","lightgbm","magrittr","data.table")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages))
  {
   install.packages(new.packages)
  }
```

```{r}
#write library
library(lightgbm)
library(magrittr)
library(data.table)
library(randomForest)
```

Step 1: Load, process and clean the data
```{r}
train_num<- fread("../output/train_num.csv") %>% as.data.frame()
test_num<- fread("../output/test_num.csv") %>% as.data.frame()
train_num_runique<- fread("../output/train_num_runique.csv") %>% as.data.frame()
test_num<- fread("../output/test_num_runique.csv") %>% as.data.frame()


test = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_test.csv") %>% as.data.frame()
train = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_train.csv") %>% as.data.frame()
people = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/people.csv") %>% as.data.frame()

names(people)[2:length(names(people))]=paste0('people_',names(people)[2:length(names(people))])
p_logi <- names(people)[which(sapply(people, is.logical))]
for (col in p_logi) {
  set(people, j = col, value = as.numeric(people[[col]]))
}
train_agg <- merge(train, people, by = "people_id", all.x = T)


```

Step 2: Implement models

Model 1: SVM
```{r}
# Linear SVM
source("/Users/yg2477/Desktop/fall2017-project5-proj5-grp3-master/lib/svm_linear.R")
# svm_linear(train_num_runique, 5)

#RBF Kernel
source("/Users/yg2477/Desktop/fall2017-project5-proj5-grp3-master/lib/svm_rbf.R")
#svm_rbf(train_num_runique, 5)
```

We can find the accuracy of linear svm is 83.09%, while the RBF Kernel behaves even better, the accuracy is 88.22%.

Model 2: Multinomial Log-Linear Model
```{r}

```

Model 3: Random Forest
```{r}

  library(randomForest)

  train_num = train_num[, !names(train_num) %in% c("V1")]
  length(train_num)
  fit <- randomForest(as.factor(outcome)~., data= train_num, mtry=8,
                      importance=TRUE, 
                      ntree=25)
  
```

Model 4: Light GBM
```{r}
DT2mat <- function(DT, low_mem = FALSE, collect = 0, silent = TRUE) {
  
  # Can't initialize lower
  mat_sub <- matrix(rep(FALSE, nrow(DT) * ncol(DT)), ncol = ncol(DT))
  
  cols <- copy(colnames(DT))
  colnames(mat_sub) <- cols
  
  if (collect == 0) {
    # Don't garbage collect
    
    if (low_mem == TRUE) {
      # delete old
      for (i in cols) {
        mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
        set(DT, j = i, value = NULL)
      }
      
    } else {
      # not low mem
      for (i in cols) {
        mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
      }
      
    }
    
  } else {
    # Do garbage collect
    
    if (silent == FALSE) {
      # not silent
      
      if (low_mem == TRUE) {
        # delete old
        j <- 1
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          set(DT, j = i, value = NULL)
          if (!(j %% collect)) {gc(verbose = FALSE); cat("\rIteration: ", j, ".", sep = "")}
        }
        
      } else {
        # not low mem
        j <- 0
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          if (!(j %% collect)) {gc(verbose = FALSE); cat("\rIteration: ", j, ".", sep = "")}
        }
        
      }
      
    } else {
      
      if (low_mem == TRUE) {
        # delete old
        j <- 1
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          set(DT, j = i, value = NULL)
          if (!(j %% collect)) {gc(verbose = FALSE)}
        }
        
      } else {
        # not fast
        j <- 0
        for (i in cols) {
          j <- j + 1
          mat_sub[, which(cols %in% i)] <- copy(DT[[i]])
          if (!(j %% collect)) {gc(verbose = FALSE)}
        }
        
      }
      
    }
  }
  
  return(mat_sub)
  
}
```

```{r}




#manual treatment:
train_num$people_id <- as.numeric(as.factor(train_num$people_id)) - 1
train_num$activity_id <- as.numeric(as.factor(train_num$activity_id)) - 1
train_num$date <- as.numeric(as.factor(train_num$date)) - 1
train_num$people_date <- as.numeric(as.factor(train_num$people_date)) - 1
train_num = train_num[, !names(train_num) %in% c("V1")]
train_num_data= train_num[, !names(train_num) %in% c("outcome")]

#automated treatment
rules <- lgb.prepare_rules(data = train_agg)
data_train <- bank_rules$data


temp_train <- lgb.Dataset(DT2mat(train_num_data), label=train_num$outcome, free_raw_data = FALSE, colnames = colnames(train_num_data), categorical_feature = c(1, 2, 3, 18))

bst <- lightgbm(data = temp_train,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
                objective = "binary")

pred <- predict(bst, DT2mat(train_num_data))
err <- mean(as.numeric(pred > 0.5) != train_num$outcome)
print(paste("test-error=", err))


print("Training lightgbm with lgb.Dataset")
dtrain <- lgb.Dataset(data = test_num[, !names(test_num) %in% c("V1", "outcome")],
                      label = test_num$outcome)
bst <- lightgbm(data = dtrain,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
objective = "binary")

bst <- lightgbm(data = as.matrix(train_num$data),
                label = train$label,
                num_leaves = 4,
                learning_rate = 1,
                nrounds = 2,
objective = "binary")
```

Model 5: Xgboost

Model 6: Neural Network