---
title: "Main Notebook"
output: html_notebook
author: group3
---

In this project, we conducted a couple of machine learning algorithms on Red Hat Business data set, which is pretty large(almost two millions*56). We mainly compared SVM(linear and nonlinear), Random Forest, Light GBM, Xgboost, Neural Network, Multinomial log-linear Model. And it proved Xgoost and Light GBM have the highest accuracy around 98%.

###Step 0: Load the packages
```{r}
list.of.packages <- c("data.table","FeatureHashing","xgboost","dplyr","Matrix","caret","randomForest","e1071","lightgbm","magrittr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages))
  {
   install.packages(new.packages)
  }

library(data.table) 
library(FeatureHashing)
library(xgboost)
library(dplyr)
library(Matrix)
library(e1071)
library(lightgbm)
library(magrittr)
library(randomForest)
```

###Step 1: Data processing: turn categorical variables into numeric variables

The process is showed in DOC file: data_processing.Rmd and data_processing_uniquea_same.Rmd and the results are in Output links (Since the limit size of file is 25MB)

In the data processing step, we converted all values of features into numeric values, and get the results : train_num. The code is in /doc/data_processing.Rmd

Since many features are categorical variables, in order to avoid the dummy variable trap, we set all frequency =1 character values in one column to be same numeric values. The code is in /doc/data_processing_uniquea_same.Rmd
```{r}
train_num<- fread("../output/train_num.csv") %>% as.data.frame()
train_num_runique<- fread("../output/train_num_runique.csv") %>% as.data.frame()
```

###Step 2: Implement models

###Model 1: SVM
```{r}
# Linear SVM
source("../lib/svm_linear.R")
# svm_linear(train_num_runique, 5)

#RBF Kernel
source("../lib/svm_rbf.R")
#svm_rbf(train_num_runique, 5)
```

We can find the accuracy of linear svm is 83.09%, while the RBF Kernel behaves even better, the accuracy is 88.22%.

###Model2: Multinomial Log Linear

library(nnet)
multinom_data <- train_num


#SUBSET TO RELEVANT VARIABLES
#unfortunately, multinom() cannot handle more than 1024 categories of a variable 
multinom_data <- subset(data,select=-c(X,people_id,activity_id,date,people_date))


#SPLIT INTO TRAIN AND TEST
set.seed(2017)
multinom_index <- sample(1:nrow(multinom_data), size=0.7*nrow(multinom_data)) 
multinom_train_data <- multinom_data[index,] 
multinom_test_data <- multinom_data[-index,]


#TRAIN THE MODEL + OBTAIN RUN TIME
multinom_train <- function(multinom_train_data){
  multinom_fit <- multinom(formula = as.factor(outcome) ~ .,
                           data=multinom_train_data, MaxNWts = 10000, maxit = 500)
  top_models = varImp(multinom_fit)
  top_models$variables = row.names(top_models)
  top_models = top_models[order(-top_models$Overall),]
  return(list(fit=multinom_fit, top=top_models))
}
system.time(multinomfit_train <- multinom_train(train_data))


#TEST THE MODEL
multinom_test <- function(multinom_test_data, fit){
  multinom_pred = predict(fit,newdata=multinom_test_data)
  return(multinom_pred)
}

Run it:
multinomtest_result = multinom_test(multinom_test_data,multinomfit_train$fit)
postResample(multinom_test_data$outcome,multinomtest_result)



###Model 3: Random Forest

We need to do some pre-processing before using the Random Forest Model. Indeed, all the variables that we use in this model are categorical variables except ‘people date’ and ‘activity date’. RF implementation in R cannot deal with categorical variables if some variables have more than 52 different labels.

Pre-processing: For this model we decided to separate ‘activitygroup’=1 with ‘activity_group’!=1, since these activities in these two groups of categories doesn’t have the same the same variables (variables for which ‘activity group’=1 has 9 additional variables). We also decided to tranform variables with more than 52 labels into numeric values.

Model: We run a Random Forest model with parameter mtrt=sqrt(44) and pruning (importance=TRUE) for bothe activity1 and without_activity1
```{r}
#trainingIndex=sample(nrow(thisdata), round((nrow(thisdata)*0.40)))
#data.test=thisdata[-trainingIndex,]
#data.train=thisdata[trainingIndex,]
#rf.model = randomForest(as.factor(outcome)~., data=thisdata, subset= trainingIndex, mtry =7, importance=TRUE)
#importance(rf.model)
#list(rf.model, data.test,trainingIndex)
#train_num = train_num[, !names(train_num) %in% c("V1")]
#length(train_num)
#fit <- randomForest(as.factor(outcome)~., data= train_num, mtry=8,
#importance=TRUE,
#ntree=25)
```



###Model 4: Light GBM

Pre-processing: We first pre-process the file separating the data set into two part, one corresponding to activity_group=1 and the other one corresponding to activity_group!=1.

```{r, warning=FALSE}
# Data Pre-processing, separation into two data set for group activity =1 and group_activity !=1
source("../lib/lightGBM.R")
#lightGBM(train_num_runique)
```

Results:

We run the Light GBM model on the first data set: activity =1 with the following parameter: max_depth = 8, learning_rate=0.1, 1500 iterations We obtain the following accuracy: train's l2: 0.0165753 valid's l2: 0.0187008, so an accuracy of 98.13% on the test set We run the code in 2.93 mins.

We then run the Light GBM model on the second data set: activity !=1 with the following parameter: max_depth = 8, learning_rate=0.1, 1500 iterations We obtain the following accuracy: train's l2:0.0264289 valid's l2:0.0285721, so an accuracy of 97.15% on the test set We run the code in 7.04 mins.

###Model 5: Xgboost

```{r}
source("/Users/linhan/Desktop/fall2017-project5-proj5-grp3/lib/xgboost.R")
```

In order to make computation more efficient, we converted all non-sparse columns to sparse matrix and rbind them with other sparse columns.

In xgboost model, I trained the model using different parameters combinations like size of each boosting step of 0.02,0.05,0.1 and maximum depth of the tree of 5,10,15. I find that when eta= 0.02, max_depth=10, the model have the highest accuracy. So I used it as the best model to do the following step---(5 folds cross-validation), and get the results:
```{r, warning=FALSE}
run.xgb<- TRUE
if(run.xgb){
 system.time(xgb(train_num_runique, K=5))
}
####Results:
# [100]	train-auc:0.990005+0.000024	test-auc:0.989440+0.000104 
#     user   system  elapsed 
#  591.506  510.861 1357.692 
```

From the results, we can see that the accuracy of Xgboost model is 98.94%. The running time is 1357.69 seconds.

###Model 6: Neural Network

The multiple-layer neural network codes is in /doc/neural_network.ipynb. We use 5 folds cross-validation to do evaluation.
The results are as following:

CV Score         hidden layer        running time

75.57%                1                 148.94s

82.03%                6                 354.62s

80.56%                10                1639.65s

83.72%                15                4380.30s
