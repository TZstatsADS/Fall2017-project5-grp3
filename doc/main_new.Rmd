---
  title: "Main Notebook"
output: html_notebook
---
  
  In this project, we conducted a couple of machine learning algorithms on Red Hat Business data set, which is pretty large(almost two millions*56). We mainly compared SVM(linear and nonlinear), Random Forest, Light GBM, Xgboost, Neural Network, Multinomial log-linear Model. And it proved Xgoost and Light GBM have the highest accuracy around 98%.

Step 0: Load the packages
```{r}
list.of.packages <- c("data.table","FeatureHashing","xgboost","dplyr","Matrix","caret","randomForest","lightgbm","magrittr","data.table") 
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages))
{
  install.packages(new.packages)
}

library(magrittr)
library(data.table)
```



Step 1: Load, process and clean the data
```{r}
train_num<- fread("../output/train_num.csv") %>% as.data.frame()
test_num<- fread("../output/test_num.csv") %>% as.data.frame()
train_num_runique<- fread("../output/train_num_runique.csv") %>% as.data.frame()
test_num<- fread("../output/test_num_runique.csv") %>% as.data.frame()


test = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_test.csv") %>% as.data.frame()
train = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/act_train.csv") %>% as.data.frame()
people = fread("/home/vassily/Desktop/Link to Fall 2017/Applied Data Science/fall2017-project5-proj5-grp3/data/people.csv") %>% as.data.frame()

names(people)[2:length(names(people))]=paste0('people_',names(people)[2:length(names(people))])
p_logi <- names(people)[which(sapply(people, is.logical))]
for (col in p_logi) {
  set(people, j = col, value = as.numeric(people[[col]]))
}
train_agg <- merge(train, people, by = "people_id", all.x = T)


```

Step 2: Implement models

Model 1: SVM
```{r}
# Linear SVM
source("/Users/yg2477/Desktop/fall2017-project5-proj5-grp3-master/lib/svm_linear.R")
# svm_linear(train_num_runique, 5)

#RBF Kernel
source("/Users/yg2477/Desktop/fall2017-project5-proj5-grp3-master/lib/svm_rbf.R")
#svm_rbf(train_num_runique, 5)
```

We can find the accuracy of linear svm is 83.09%, while the RBF Kernel behaves even better, the accuracy is 88.22%.

Model 2: Multinomial Log-Linear Model
```{r}

```

Model 3: Random Forest
```{r}

library(randomForest)

train_num = train_num[, !names(train_num) %in% c("V1")]
length(train_num)
fit <- randomForest(as.factor(outcome)~., data= train_num, mtry=8,
                    importance=TRUE, 
                    ntree=25)

```

Model 4: Light GBM
```{r}
# Data Pre-processing, separation into two data set for group activity =1 and group_activity !=1
source("../lib/lightGBM.R")
#lightGBM(train_num_runique)
#Warning: to use this model, please install the lightGBM library following the instruction on this link https://github.com/Microsoft/LightGBM/tree/master/R-package

```

#Pre-processing:
We first pre-process the file separating the data set into two part, one corresponding to activity_group=1 and the other one corresponding to activity_group!=1

RESULTS:

We run the Light GBM model on the first data set: activity =1 with the following parameter: max_depth = 8, learning_rate=0.1, 1500 iterations
We obtain the following accuracy:
train's l2: 0.0165753	
valid's l2: 0.0187008, so an accuracy of 98.13% on the test set
We run the code in 2.93 mins

We then run the Light GBM model on the second data set: activity !=1 with the following parameter: max_depth = 8, learning_rate=0.1, 1500 iterations
We obtain the following accuracy:
train's l2:0.0264289	
valid's l2:0.0285721, so an accuracy of 97.15% on the test set
We run the code in 7.04 mins



  ###
  

Model 5: Xgboost

Model 6: Neural Network